训练的脚本文件在根目录下，名字为TrainCpt.py
使用步骤：

1。由于这个脚本是基于spark写的（如果不使用Spark，在30w数据量的前提下，运算的速度实在是太慢了，可能算一个节点就要超过10小时。。。
ps：我也没试过，但是可以想象），因此，需要安装以下几个东西：
    (1).java
    (2).scala
    (3)hadoop
    (4)spark
    安装和配置环境变量的方法网上一搜索就有了，所以这个文件里就不详细写了，如果配置不对的话我可以远程帮忙^_^

2.找到spark的安装目录，里面有一个./bin/spark-submit文件，我们的命令行执行就需要用到这个文件

3.使用命令行（或者可以查一下R语言里是怎么调用命令行的，应该有详细的办法，找到之后就可以把这个脚本的执行写到你的R脚本中，
因为我对这个语言不是特别熟悉，所以就只能师姐自己查啦）命令为：
    ./bin/spark-submit xxx/xxx/xxx/trainCpt.py(这个是你放本脚本文件的具体目录位置) 参数1 参数2 参数3 参数4
其中，参数1，2，3，4分别为：
    参数1：你数据文件存放的路径
    参数2：输出CPT的文件路径，这个文件路径不能只是目录，要包含你想要命名的具体的文件名，比如：./xxx/xxx/result.txt
    参数3：你要计算的CPT的节点的索引值，索引对应于数据文件title，从0开始，比如你现在要计算cpuMax，那么这个传值就应该是0，要计算
    cpuAve，那么传值1，这个参数可以是数组，因为你可能一个节点是由多个数字组成，如果是这种情况，那么你可以传对应与数据文件的矩阵的列号，
    不同列之间用"，"分割，比如你可以传："1，2，5，67，7，4"
    参数4：你要计算的这个节点的所有父节点的列序号索引。同样的，如果这个节点有多个父节点或者一个父节点中有多个数组组成，那么你也可以根据你
    的需要传类似"1，2，3，4，5，6，7..."这样的列表，每个列序号之间用逗号分割，同参数3

4.在脚本执行完成之后，会在你指定的输出文件路径中生成你要的文件，比如你指定./xxx/xxx/result.txt作为输出文件，那么程序结束之后就会生成
这个文件，这个文件在每一次生成的时候会自动覆盖之前的文件，如果这个文件不存在也会自动创建，不用每次都去手动创建。同时，在你指定的输出文件的
同目录下会生成一个info.txt文件，这个文件的第一行表示计算出的CPT结果文件中每一行对应的是什么，用空格分割，这个文件的第二行表示CPT结果文件
的每一列对应的title是什么，同样也是用空格分割

5.生成的CPT结果文件中，是一个矩阵，矩阵的每一行代表的是当前你要计算的这个节点的状态取值，这个取值具体是是什么可以查看info.txt文件，是一一
对应的，同样的，矩阵的列表示的是这个节点父亲节点的组合的状态取值，与info.txt文件中给出的状态取值也是一一对应的
（具体的info.txt文件样例和数据结果样例，可以参考本工程下data目录中的info.txt和result文件）

6.我测试仅仅使用了8000+条数据，其计算时间差不多要半分钟（还是在使用spark伪分布式的情况下），如果使用全部的数据集，预计要计算的时间很久，
建议使用我们的平台进行计算
